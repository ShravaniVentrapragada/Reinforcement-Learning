# -*- coding: utf-8 -*-
"""SmartTaxi_QLearning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hP8BWxGT7kMeLCoDL5_SVJezO5FB0gu0

# ***Teach a Taxi to pick up and drop off passengers at the right locations with Reinforcement Learning***

Name: Snigdha Labh

PRN:17070123105

G-5
"""

import gym
import numpy as np
import pickle, os

env = gym.make("Taxi-v3")
env

state = env.reset()
state

env.observation_space.n

env.render()

state

env.render()

"""<h1>Possible Actions</h1>

down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)
"""

n_states = env.observation_space.n
n_actions = env.action_space.n

n_actions

env.env.s = 150

env.render()

env.step(1)

"""<h1>How good does behaving completely random do?</h1>

# ***Presetting the variables***
"""

state = env.reset()
counter = 0
g = 0
reward = None

while reward != 20:
    state, reward, done, info = env.step(env.action_space.sample())
    counter += 1
    g += reward

print("Solved in {} Steps with a total reward of {}".format(counter,g))

"""## Let's look at just one episode and see how the Q values change after each step using the formula below"""

Q = np.zeros([n_states, n_actions])

Q

episodes = 1
G = 0
alpha = 0.618

for episode in range(1,episodes+1):
    done = False
    G, reward = 0,0
    state = env.reset()
    firstState = state
    print("Initial State = {}".format(state))
    while reward != 20:
        action = np.argmax(Q[state]) 
        state2, reward, done, info = env.step(action)
        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) 
        G += reward
        state = state2

finalState = state

finalState

"""## Let's look at the first step:"""

firstState

"""## Let's look at the final step:"""

finalState

Q

"""## Let's run over multiple episodes so that we can converge on a optimal policy"""

episodes = 5000
rewardTracker = []

G = 0
alpha = 0.618

for episode in range(1,episodes+1):
    done = False
    G, reward = 0,0
    state = env.reset()
    while done != True:
        action = np.argmax(Q[state]) 
        state2, reward, done, info = env.step(action) 
        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))
        G += reward
        state = state2
        
    if episode % 100 == 0:
        print('Episode {} Total Reward: {}'.format(episode,G))

rewardTracker

"""## Now that we have learned the optimal Q Values we have developed a optimal policy and have no need to train the agent anymore"""

state = env.reset()
done = None

while done != True:
    # We simply take the action with the highest Q Value
    action = np.argmax(Q[state])
    state, reward, done, info = env.step(action)
    env.render()

with open("smartTaxi_qTable.pkl", 'wb') as f:
    pickle.dump(Q, f)

with open("smartTaxi_qTable.pkl", 'rb') as f:
    Qtest = pickle.load(f)

!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py
from colab_pdf import colab_pdf
colab_pdf('SmartTaxi_QLearning.ipynb')